{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Song Popularity Prediction Using Machine Learning","metadata":{}},{"cell_type":"markdown","source":"### Importing Data and Libraries","metadata":{}},{"cell_type":"code","source":"# Import Statements\nimport numpy as np # Linear algebra and pandas compatibility\nimport pandas as pd # Data management, and dataframes\nfrom sklearn.model_selection import train_test_split # Splits dataset into training|testing sets\nfrom sklearn.tree import DecisionTreeRegressor # Decision Tree Model\nfrom sklearn.ensemble import RandomForestRegressor # Random Forest Model\nfrom sklearn.metrics import mean_absolute_error # MAE, measuring loss\nimport matplotlib.pyplot as plt # Graphing library to visualize the data/correlations\nimport seaborn as sns #Heatmap\nfrom yellowbrick.model_selection import FeatureImportances # Correlation finding\nfrom sklearn.linear_model import LogisticRegression\nfrom yellowbrick.datasets import load_energy\nfrom yellowbrick.model_selection import ValidationCurve","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-09T00:16:47.043022Z","iopub.execute_input":"2021-07-09T00:16:47.043406Z","iopub.status.idle":"2021-07-09T00:16:47.051185Z","shell.execute_reply.started":"2021-07-09T00:16:47.04337Z","shell.execute_reply":"2021-07-09T00:16:47.049972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing our dataset: \"Spotify dataset 1922-2021\"\nfile_path = '../input/spotify-dataset-19212020-160k-tracks/data_o.csv' # CSV file\ndf = pd.read_csv(file_path) # Creating our main dataframe named \"df\" using pd.read_csv","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:47.052945Z","iopub.execute_input":"2021-07-09T00:16:47.053351Z","iopub.status.idle":"2021-07-09T00:16:47.977767Z","shell.execute_reply.started":"2021-07-09T00:16:47.053315Z","shell.execute_reply":"2021-07-09T00:16:47.976733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Our Data","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:47.979525Z","iopub.execute_input":"2021-07-09T00:16:47.979836Z","iopub.status.idle":"2021-07-09T00:16:47.985411Z","shell.execute_reply.started":"2021-07-09T00:16:47.979806Z","shell.execute_reply":"2021-07-09T00:16:47.984327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:47.986981Z","iopub.execute_input":"2021-07-09T00:16:47.987312Z","iopub.status.idle":"2021-07-09T00:16:48.01668Z","shell.execute_reply.started":"2021-07-09T00:16:47.987281Z","shell.execute_reply":"2021-07-09T00:16:48.015523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:48.018192Z","iopub.execute_input":"2021-07-09T00:16:48.018589Z","iopub.status.idle":"2021-07-09T00:16:48.179207Z","shell.execute_reply.started":"2021-07-09T00:16:48.018557Z","shell.execute_reply":"2021-07-09T00:16:48.17825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:48.180269Z","iopub.execute_input":"2021-07-09T00:16:48.180541Z","iopub.status.idle":"2021-07-09T00:16:48.26914Z","shell.execute_reply.started":"2021-07-09T00:16:48.180508Z","shell.execute_reply":"2021-07-09T00:16:48.268109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looking for Obvious Trends","metadata":{}},{"cell_type":"code","source":"Benchmark = df[['artists','year','name', 'release_date', 'popularity']]\n# The Benchmark is the \"Popularity\" index, as that is our label\nBenchmark = Benchmark.sort_values(by=['popularity', 'name'], ascending=[False, True])\nBenchmark.artists = Benchmark.artists.str.strip('[]').str.replace(\"'\", \"\")\nBenchmark.head(50)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:48.271506Z","iopub.execute_input":"2021-07-09T00:16:48.271789Z","iopub.status.idle":"2021-07-09T00:16:49.021823Z","shell.execute_reply.started":"2021-07-09T00:16:48.271761Z","shell.execute_reply":"2021-07-09T00:16:49.020867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In the previous, it seems as though 2020 (newest) is favored. Is the opposite true for the oldest?\nyear = Benchmark.sort_values(by=['year', 'popularity'], ascending=[True, False])\nyear.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:49.023542Z","iopub.execute_input":"2021-07-09T00:16:49.023837Z","iopub.status.idle":"2021-07-09T00:16:49.067144Z","shell.execute_reply.started":"2021-07-09T00:16:49.023807Z","shell.execute_reply":"2021-07-09T00:16:49.0663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perhaps this is biased, as the music from 1921 might actually be less popular. Let's use qualitative data to see if our hypothesis of new year = +popularity is true.\npopularoldyear = df.loc[(df['year'] >= 1950) & (df['year'] <= 2000)]\npopularoldyear = popularoldyear.sort_values(by=['popularity', 'name'], ascending=[False, True])\npopularoldyear.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:49.068366Z","iopub.execute_input":"2021-07-09T00:16:49.068738Z","iopub.status.idle":"2021-07-09T00:16:49.379748Z","shell.execute_reply.started":"2021-07-09T00:16:49.068699Z","shell.execute_reply":"2021-07-09T00:16:49.378744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now that we have a better idea as to how Spotify ranks the popularity of their music, we can start analyzing its correlation with other features on the Spotify API dataset.","metadata":{}},{"cell_type":"code","source":"# Creating a simple heatmap to see the correlation of our features to our labels\nsns.heatmap(df.corr(), cmap='icefire');\n# With this heatmap, we can decide which values are most valuable (usually the close to 1.0, the better)\n# We can also decide which ones we might want to make synthetic features with.","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:49.380955Z","iopub.execute_input":"2021-07-09T00:16:49.381241Z","iopub.status.idle":"2021-07-09T00:16:49.918977Z","shell.execute_reply.started":"2021-07-09T00:16:49.381201Z","shell.execute_reply":"2021-07-09T00:16:49.917874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at this graph, there are a couple pairs which have great correlation:\n- Year - Popularity (Positive)\n- Loudness - Energy (Positive)\n- Energy - Acousticness (Negative)\n- Acousticness - Year (Negative)\n\n##### The only ones of these pairs that could give us meaningful synthetic data are Loudness - Energy, and Energy - Acousticness as the first includes the label, and the last includes our main feature, \"year\".\n\nAlso, mode and key have very little correlation with any values, we can remove them.\n","metadata":{}},{"cell_type":"code","source":"df.corr()['popularity'].sort_values(ascending=False) # Numerical values to prove hypotheses","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:49.920188Z","iopub.execute_input":"2021-07-09T00:16:49.920501Z","iopub.status.idle":"2021-07-09T00:16:50.047364Z","shell.execute_reply.started":"2021-07-09T00:16:49.920472Z","shell.execute_reply":"2021-07-09T00:16:50.04645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()['energy'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:50.048417Z","iopub.execute_input":"2021-07-09T00:16:50.048742Z","iopub.status.idle":"2021-07-09T00:16:50.175201Z","shell.execute_reply.started":"2021-07-09T00:16:50.048704Z","shell.execute_reply":"2021-07-09T00:16:50.17424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing our main feature\ny = df.popularity\n \nfig,axs = plt.subplots(2,1, figsize=(7,7))\nfig.suptitle('Observations of popularity')\n \n# Observe the distribution of 'popularity'axs[0].set_title('Distribution of popularity')\naxs[0].set_title('Distribution of popularity')\nsns.distplot(df['popularity'], ax=axs[0], kde=False)\n \naxs[1].set_title('Relationship between popularity and year')\nsns.lineplot(x='year', y='popularity', data=df, ax=axs[1])\n \nfig.tight_layout(pad=3.0)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:50.177727Z","iopub.execute_input":"2021-07-09T00:16:50.178025Z","iopub.status.idle":"2021-07-09T00:16:55.501169Z","shell.execute_reply.started":"2021-07-09T00:16:50.177997Z","shell.execute_reply":"2021-07-09T00:16:55.500195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the predictor variables\ndf[\"loud_energy\"] = df[\"energy\"] * df[\"loudness\"]\ndf[\"acoustic_energy\"] = df['acousticness'] * df['energy']\nfeatures = ['valence', 'acousticness', 'danceability',\n       'duration_ms', 'energy', 'explicit', 'year', 'instrumentalness',\n       'liveness', 'loudness', 'speechiness', 'tempo', 'loud_energy', 'acoustic_energy']\n# Removed 'mode' and 'key', as they had little/no correllation with popularity.\nX = df[features]\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:55.502465Z","iopub.execute_input":"2021-07-09T00:16:55.502779Z","iopub.status.idle":"2021-07-09T00:16:55.543558Z","shell.execute_reply.started":"2021-07-09T00:16:55.502747Z","shell.execute_reply":"2021-07-09T00:16:55.542523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X, test_X, train_y, test_y = train_test_split(X,y, test_size= 0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:55.544624Z","iopub.execute_input":"2021-07-09T00:16:55.544912Z","iopub.status.idle":"2021-07-09T00:16:55.589706Z","shell.execute_reply.started":"2021-07-09T00:16:55.544879Z","shell.execute_reply":"2021-07-09T00:16:55.588663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training, Testing, and Predicting","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor()\nviz = FeatureImportances(model)\nviz.fit(train_X, train_y)\nviz.show()\n# Another correlation analysis, now using Machine Learning scripts to get a better understadning.","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:16:55.591019Z","iopub.execute_input":"2021-07-09T00:16:55.591442Z","iopub.status.idle":"2021-07-09T00:20:40.111902Z","shell.execute_reply.started":"2021-07-09T00:16:55.591402Z","shell.execute_reply":"2021-07-09T00:20:40.110624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Comparing Models","metadata":{}},{"cell_type":"code","source":"lr_model = LogisticRegression(random_state=0)\nlr_model.fit(train_X,train_y)\nval_preds1 = lr_model.predict(test_X)\nval_mae1 = mean_absolute_error(test_y, val_preds1)\nprint(f'Mean absolute error of this model: {val_mae1:.3f}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:20:40.113332Z","iopub.execute_input":"2021-07-09T00:20:40.113756Z","iopub.status.idle":"2021-07-09T00:22:20.684969Z","shell.execute_reply.started":"2021-07-09T00:20:40.11372Z","shell.execute_reply":"2021-07-09T00:22:20.683879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_tree = DecisionTreeRegressor(random_state=0)\ndec_tree.fit(train_X, train_y)\nval_preds1 = dec_tree.predict(test_X)\nval_mae2 = mean_absolute_error(test_y, val_preds1)\nprint(f'Mean absolute error of this model: {val_mae2:.3f}')\n# print(f'Training Coefficient of R^2 : {dec_tree.score(train_X, train_y):.2f}')\n# print(f'Test Coefficient of R^2 : {dec_tree.score(test_X, test_y):.2f}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:22:20.686667Z","iopub.execute_input":"2021-07-09T00:22:20.687326Z","iopub.status.idle":"2021-07-09T00:22:24.433319Z","shell.execute_reply.started":"2021-07-09T00:22:20.687274Z","shell.execute_reply":"2021-07-09T00:22:24.432211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model = RandomForestRegressor(random_state=0)\nrf_model.fit(train_X, train_y)\nval_preds1 = rf_model.predict(test_X)\nval_mae3 = mean_absolute_error(test_y, val_preds1)\nprint(f'Mean absolute error of this model: {val_mae3:.3f}')\n# Unfortunately, due to hardware limitaions, I couldn't include a validation curve to check for overfitting.","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:22:24.436098Z","iopub.execute_input":"2021-07-09T00:22:24.436442Z","iopub.status.idle":"2021-07-09T00:26:12.780213Z","shell.execute_reply.started":"2021-07-09T00:22:24.436399Z","shell.execute_reply":"2021-07-09T00:26:12.779447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Addressing a large problem in the dataset: The Year Bias","metadata":{}},{"cell_type":"code","source":"df[\"loud_energy\"] = df[\"energy\"] * df[\"loudness\"]\ndf[\"acoustic_energy\"] = df['acousticness'] * df['energy']\nfeatures = ['valence', 'acousticness', 'danceability',\n       'duration_ms', 'energy', 'explicit', 'instrumentalness',\n       'liveness', 'loudness', 'speechiness', 'tempo', 'loud_energy', 'acoustic_energy']\n# REMOVED YEAR\nX = df[features]\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:26:12.782253Z","iopub.execute_input":"2021-07-09T00:26:12.782914Z","iopub.status.idle":"2021-07-09T00:26:12.834957Z","shell.execute_reply.started":"2021-07-09T00:26:12.782866Z","shell.execute_reply":"2021-07-09T00:26:12.834116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X, test_X, train_y, test_y = train_test_split(X,y, test_size= 0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:26:12.837897Z","iopub.execute_input":"2021-07-09T00:26:12.83918Z","iopub.status.idle":"2021-07-09T00:26:12.913606Z","shell.execute_reply.started":"2021-07-09T00:26:12.839038Z","shell.execute_reply":"2021-07-09T00:26:12.912775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model = LogisticRegression(random_state=0)\nlr_model.fit(train_X,train_y)\nval_preds1 = lr_model.predict(test_X)\nval_mae4 = mean_absolute_error(test_y, val_preds1)\nprint(f'Mean absolute error of this model: {val_mae4:.3f}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:26:12.914721Z","iopub.execute_input":"2021-07-09T00:26:12.91518Z","iopub.status.idle":"2021-07-09T00:27:56.243918Z","shell.execute_reply.started":"2021-07-09T00:26:12.915147Z","shell.execute_reply":"2021-07-09T00:27:56.242627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:27:56.246278Z","iopub.execute_input":"2021-07-09T00:27:56.247136Z","iopub.status.idle":"2021-07-09T00:27:56.290194Z","shell.execute_reply.started":"2021-07-09T00:27:56.247077Z","shell.execute_reply":"2021-07-09T00:27:56.288728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_tree = DecisionTreeRegressor(random_state=0)\ndec_tree.fit(train_X, train_y)\nval_preds1 = dec_tree.predict(test_X)\nval_mae5 = mean_absolute_error(test_y, val_preds1)\nprint(f'Mean absolute error of this model: {val_mae5:.3f}')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:27:56.296565Z","iopub.execute_input":"2021-07-09T00:27:56.29711Z","iopub.status.idle":"2021-07-09T00:27:59.848314Z","shell.execute_reply.started":"2021-07-09T00:27:56.297061Z","shell.execute_reply":"2021-07-09T00:27:59.84708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model = RandomForestRegressor(random_state=0)\nrf_model.fit(train_X, train_y)\nval_preds1 = rf_model.predict(test_X)\nval_mae6 = mean_absolute_error(test_y, val_preds1)\nprint(f'Mean absolute error of this model: {val_mae6:.3f}')\n# Unfortunately, due to hardware limitaions, I couldn't include a validation curve to check for overfitting.","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:27:59.84988Z","iopub.execute_input":"2021-07-09T00:27:59.850338Z","iopub.status.idle":"2021-07-09T00:31:35.459696Z","shell.execute_reply.started":"2021-07-09T00:27:59.850288Z","shell.execute_reply":"2021-07-09T00:31:35.458615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [[ 31.641, 9.281,  6.753]]\n\ncolumns = ('LogisticRegression', 'DecisionTree', 'RandomForest')\nrows = ['%d Mean Absolute Error' % x for x in (100, 50, 20, 10, 5)]\n\nvalues = np.arange(0, 2500, 500)\nvalue_increment = 1000\n\n# Get some pastel shades for the colors\ncolors = plt.cm.magma(np.linspace(0, 0.5, len(rows)))\nn_rows = len(data)\n\nindex = np.arange(len(columns)) + 0.3\nbar_width = 0.4\n\n# Initialize the vertical-offset for the stacked bar chart.\ny_offset = np.zeros(len(columns))\n\n# Plot bars and create text labels for the table\ncell_text = []\nfor row in range(n_rows):\n    plt.bar(index, data[row], bar_width, bottom=y_offset, color=colors[row])\n    y_offset = y_offset + data[row]\n    cell_text.append(['%1.1f' % (x / 1000.0) for x in y_offset])\n# Reverse colors and text labels to display the last value at the top.\ncolors = colors[::-1]\ncell_text.reverse()\n\n# Add a table at the bottom of the axes\nthe_table = plt.table(cellText=cell_text,\n                      rowLabels=rows,\n                      rowColours=colors,\n                      colLabels=columns,\n                      loc='bottom')\n\n# Adjust layout to make room for the table:\nplt.subplots_adjust(left=0.2, bottom=0.2)\n\nplt.ylabel(\"Loss in ${0}'s\".format(value_increment))\nplt.yticks(values * value_increment, ['%d' % val for val in values])\nplt.xticks([])\nplt.title('Loss by Disaster')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T00:31:35.461034Z","iopub.execute_input":"2021-07-09T00:31:35.461337Z","iopub.status.idle":"2021-07-09T00:31:35.645791Z","shell.execute_reply.started":"2021-07-09T00:31:35.461308Z","shell.execute_reply":"2021-07-09T00:31:35.644427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [[ 31.634, 14.183, 10.441]]\n\ncolumns = ('LogisticRegression', 'DecisionTree', 'RandomForest')\nrows = ['%d Mean Absolute Error' % x for x in (100, 50, 20, 10, 5)]\n\nvalues = np.arange(0, 2500, 500)\nvalue_increment = 1000\n\n# Get some pastel shades for the colors\ncolors = plt.cm.magma(np.linspace(0, 0.5, len(rows)))\nn_rows = len(data)\n\nindex = np.arange(len(columns)) + 0.3\nbar_width = 0.4\n\n# Initialize the vertical-offset for the stacked bar chart.\ny_offset = np.zeros(len(columns))\n\n# Plot bars and create text labels for the table\ncell_text = []\nfor row in range(n_rows):\n    plt.bar(index, data[row], bar_width, bottom=y_offset, color=colors[row])\n    y_offset = y_offset + data[row]\n    cell_text.append(['%1.1f' % (x / 1000.0) for x in y_offset])\n# Reverse colors and text labels to display the last value at the top.\ncolors = colors[::-1]\ncell_text.reverse()\n\n# Add a table at the bottom of the axes\nthe_table = plt.table(cellText=cell_text,\n                      rowLabels=rows,\n                      rowColours=colors,\n                      colLabels=columns,\n                      loc='bottom')\n\n# Adjust layout to make room for the table:\nplt.subplots_adjust(left=0.2, bottom=0.2)\n\nplt.ylabel(\"Loss in ${0}'s\".format(value_increment))\nplt.yticks(values * value_increment, ['%d' % val for val in values])\nplt.xticks([])\nplt.title('Loss by Disaster')\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-09T00:31:35.646805Z","iopub.status.idle":"2021-07-09T00:31:35.647256Z"},"trusted":true},"execution_count":null,"outputs":[]}]}